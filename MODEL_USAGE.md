# ðŸ”„ Model Workflow Summary

## Quick Reference: How the Model is Used

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VIDEO UPLOAD (MP4/AVI/etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREPROCESSING PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Extract Frames (OpenCV)                                     â”‚
â”‚  2. Detect Face Landmarks (MediaPipe Face Mesh)                 â”‚
â”‚  3. Crop Lip Region (landmarks 61-88)                           â”‚
â”‚  4. Resize to 128x128 grayscale                                 â”‚
â”‚  5. Group into chunks of 15 frames                              â”‚
â”‚  6. Normalize to [0, 1]                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT TENSOR CREATION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Shape: (N, 15, 128, 128, 1)                                   â”‚
â”‚                                                                 â”‚
â”‚  Where:                                                         â”‚
â”‚    N = Number of chunks                                         â”‚
â”‚    15 = Frames per chunk (~1 second)                            â”‚
â”‚    128 = Width/Height of lip image                              â”‚
â”‚    1 = Grayscale channel                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODEL: cremad_model_finetuned_5600_5953.keras      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Architecture (Expected):                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  TimeDistributed CNN Layers                          â”‚      â”‚
â”‚  â”‚    â†“ (Extract spatial features from each frame)      â”‚      â”‚
â”‚  â”‚  Bidirectional LSTM Layers                           â”‚      â”‚
â”‚  â”‚    â†“ (Model temporal dependencies)                   â”‚      â”‚
â”‚  â”‚  Dense + Sigmoid Output                              â”‚      â”‚
â”‚  â”‚    â†“ (Binary classification: REAL vs FAKE)           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                 â”‚
â”‚  Input:  (N, 15, 128, 128, 1)                                  â”‚
â”‚  Output: (N, 1) - probability scores                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREDICTION & CLASSIFICATION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  For each chunk:                                                â”‚
â”‚    confidence_score = model.predict(chunk)                      â”‚
â”‚                                                                 â”‚
â”‚    if confidence_score >= 0.5:                                  â”‚
â”‚        label = "REAL"                                           â”‚
â”‚    else:                                                        â”‚
â”‚        label = "FAKE"                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGGREGATION & RESULTS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Per-chunk results:                                             â”‚
â”‚    â€¢ Time range (start, end)                                    â”‚
â”‚    â€¢ Confidence score                                           â”‚
â”‚    â€¢ Label (REAL/FAKE)                                          â”‚
â”‚    â€¢ Keyframe image                                             â”‚
â”‚                                                                 â”‚
â”‚  Final verdict:                                                 â”‚
â”‚    â€¢ Majority voting across all chunks                          â”‚
â”‚    â€¢ Average confidence                                         â”‚
â”‚    â€¢ Processing time                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DASHBOARD VISUALIZATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Video player with color-coded timeline                       â”‚
â”‚  â€¢ Chunk-wise analysis table                                    â”‚
â”‚  â€¢ Confidence line chart                                        â”‚
â”‚  â€¢ Extracted lip frames grid                                    â”‚
â”‚  â€¢ Audio waveform with overlays                                 â”‚
â”‚  â€¢ Final summary card                                           â”‚
â”‚  â€¢ CSV report download                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model File Location

```
ðŸ“ deepfake_dashboard/
    ðŸ“ models/
        ðŸ“„ cremad_model_finetuned_5600_5953.keras  â† YOUR MODEL HERE
```

**Status**: âš ï¸ Currently using fallback dummy model (outputs 0.5 for all chunks)

---

## To Use Your Trained Model

### Step 1: Place Model File
```bash
# Copy your trained model to:
deepfake_dashboard/models/cremad_model_finetuned_5600_5953.keras

# Or rename your model and update app.py line 60
```

### Step 2: Verify Model Compatibility
Your model should:
- âœ… Accept input shape: `(None, 15, 128, 128, 1)`
- âœ… Output shape: `(None, 1)` or `(None,)`
- âœ… Output range: 0.0 to 1.0 (sigmoid activation)
- âœ… Be saved in Keras format (`.keras` or `.h5`)

### Step 3: Test
```bash
python app.py
# Upload a test video
# Check if predictions are reasonable (not all 0.5)
```

---

## Model Input Details

### Frame Processing
```python
# Each video frame undergoes:
1. Face detection (MediaPipe)
2. Lip landmark extraction (61-88)
3. Bounding box expansion (1.6x)
4. Crop & resize to 128x128
5. Convert to grayscale
6. Normalize to [0, 1]
```

### Chunk Creation
```python
# Frames grouped into temporal sequences:
- Chunk size: 15 frames
- ~1 second duration at 15fps
- Shape per chunk: (15, 128, 128, 1)
```

### Batch Processing
```python
# Multiple chunks processed together:
X_frames = np.stack(chunks, axis=0)
# Final shape: (N_chunks, 15, 128, 128, 1)

# Model prediction:
predictions = model.predict(X_frames)
# Output: (N_chunks, 1) probability scores
```

---

## Classification Logic

```python
# Per chunk:
if prediction >= 0.5:
    label = "REAL"     # Authentic video
else:
    label = "FAKE"     # Deepfake detected

# Final verdict (majority voting):
real_chunks = count(predictions >= 0.5)
fake_chunks = count(predictions < 0.5)

if real_chunks >= fake_chunks:
    final_verdict = "REAL"
else:
    final_verdict = "FAKE"
```

---

## Alternative: Use Your Own Model Architecture

If your model has a different architecture, modify these sections in `app.py`:

### 1. Input Preprocessing (Lines 150-200)
```python
# Change image size
lip_roi = cv2.resize(lip_roi, (YOUR_WIDTH, YOUR_HEIGHT))

# Change chunk size
CHUNK_SIZE_FRAMES = YOUR_CHUNK_SIZE
```

### 2. Model Input Format (Lines 195-210)
```python
# Example: RGB instead of grayscale
lip_roi_rgb = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2RGB)
frames_current_chunk.append(lip_roi_rgb)

# Later:
arr = np.expand_dims(arr, -1)  # Remove this for RGB (already 3 channels)
```

### 3. Multimodal Input (Lines 255-265)
```python
# If your model uses both video + audio:
preds = model.predict([X_frames, mfcc_chunks], verbose=0)
```

---

## Current Model: CREMA-D Based

**CREMA-D** = Crowd-sourced Emotional Multimodal Actors Dataset
- Originally for emotion recognition
- Fine-tuned for deepfake detection
- File suffix `5600_5953` likely indicates:
  - Training epochs, or
  - Parameter count, or
  - Dataset size

**Your model**: Replace with your actual trained model for real predictions!

---

## Questions?

1. **Where is the model loaded?** â†’ `app.py`, line 78-90
2. **What if model is missing?** â†’ Dummy model with 0.5 predictions (line 85-92)
3. **How to change threshold?** â†’ Line 272: `label = "REAL" if p >= 0.5 else "FAKE"`
4. **How to use audio?** â†’ MFCCs computed but not used; add to prediction call
5. **Model format?** â†’ Keras `.keras` or `.h5` format

---

See `MODEL_DETAILS.md` for complete technical documentation!
